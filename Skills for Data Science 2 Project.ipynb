{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1920-DSS2100 Skills for Data Science 2: Project "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is a write-up of a project I undertook for my 'Skills for Data Science 2: Project' module in second year Arts with Data Science at NUIG.\n",
    "This semester, I improved my skills in programming with Python with particular respect to the Pandas module and various NLP modules. I primarily learned through the online-learning website DataCamp.com.\n",
    "I chose to complete and write up the project \"Extract Stock Sentiment from News Headlines\" on DataCamp for this module, as I wanted to develop my NLP skills in particular.\n",
    "\n",
    "In this project I use both natural language processing methods and dataframe manipulation in Pandas to analyse stock sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import BeautifulSoup, which we shall use to obtain our data.\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "\n",
    "\n",
    "# Initialize our dictionary, which we will use to create our dataset.\n",
    "html_tables = {}\n",
    "\n",
    "# Create a for loop that will iterate over our datasets and fill our dictionary with tables and their corresponding names.\n",
    "for table_name in os.listdir('datasets'):\n",
    "    table_path = f'datasets/{table_name}'\n",
    "    table_file = open(table_path, 'r')\n",
    "    # Open this table using BeautifulSoup and read it into a variable 'html'.\n",
    "    html = BeautifulSoup(table_file)\n",
    "    # Find all tables with the id 'news-table' as this is the data that we are interested in.\n",
    "    html_table = html.find(id='news-table')\n",
    "    # Add this table to our dictionary with value 'html_table'.\n",
    "    html_tables[table_name] = html_table\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code is using BeautifulSoup to acquire our data. This step is important to get correct as it is the foundational data that we will be using throughout the rest of this code. Using BeautifulSoup gives us access to tools that allow us to quickly sift through the documents and locate and store the data we want to use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this particular instance, we used the code to grab news headlines from HTML files for stocks from FINVIZ.com. For the next step, a small bit of exploratory data analysis is required in order to develop an understanding of what data we are handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Read a single day of headlines, to keep it simple.\n",
    "tsla = html_tables['tsla_22sep.html']\n",
    "# Get all the table rows tagged in HTML with <tr> into 'tsla_tr' using BeautifulSoup.\n",
    "tsla_tr = tsla.findAll('tr')\n",
    "\n",
    "# For each row in our BeautifulSoup object:\n",
    "for i, table_row in enumerate(tsla_tr):\n",
    "    \n",
    "    # Read the text of the element 'a' into a variable.\n",
    "    link_text = table_row.a.get_text()\n",
    "    # Read the text of the element 'td' into a variable.\n",
    "    data_text = table_row.td.get_text()\n",
    "    # Print the count.\n",
    "    print(f'File number {i+1}:')\n",
    "    # Print the contents of 'link_text' and 'data_text'.\n",
    "    print(link_text)\n",
    "    print(data_text)\n",
    "    # Prevent spam, plus this is enough data to get an idea of what we are dealing with.\n",
    "    if i == 3:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from our output that our data has a few main components, such as a headline and a timestamp. The next goal is to parse this data and extract the news headlines, as this is the data we want to use for our stock sentiment analysis. We do so using the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize an empty list to hold our parsed news.\n",
    "parsed_news = []\n",
    "\n",
    "# Iterate through the news in the data we obtained.\n",
    "for file_name, news_table in html_tables.items():\n",
    "    \n",
    "    # Iterate through all the elements with the tr tag in 'news_table'.\n",
    "    for x in news_table.findAll('tr'):\n",
    "        # Read the text in each element with tag 'tr' into text.\n",
    "        text = x.get_text() \n",
    "        # Split the text in the td tag into a list. \n",
    "        date_scrape = x.td.text.split()\n",
    "        # If the length of 'date_scrape' is 1, load 'time' as the only element.\n",
    "        # If not, load 'date' as the 1st element and 'time' as the second.\n",
    "        # Numerous articles can be released in one day. This is why this condition is necessary.\n",
    "        if len(date_scrape) == 1:\n",
    "            time = date_scrape[0]\n",
    "        else:\n",
    "            date = date_scrape[0]\n",
    "            time = date_scrape[1]\n",
    "            \n",
    "        # Extract the ticker from the file name, get the string up to the 1st '_'.  \n",
    "        ticker = file_name.split('_')[0]\n",
    "        # Append ticker, date, time and headline as a list to the 'parsed_news' list.\n",
    "        parsed_news.append([ticker, date, time, x.a.get_text()])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now generated a list of lists which we will use to create our dataframe, which will become the basis of our stock sentiment analysis. We have the basis for the data that we are going to be analysing, now its time to get started on the tools that we'll be using."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our next task is to import SentimentIntensityAnalyzer from nltk.vader.sentiment and to update its lexicon in such a way that it is thinking more like a financial journalist. This last part is crucial to the sentiment analysis: We don't want our algorithm to misinterprete headlines due to a misunderstanding of the meaning behind the words used. We assign values to certain words that the algorithm will then be able to use to assign polarity scores to the news headlines for us using the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# The algorithm we are using to conduct the sentiment analysis.\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# The new words and values that we are going to teach the algorithm.\n",
    "new_words = {\n",
    "    'crushes': 10,\n",
    "    'beats': 5,\n",
    "    'misses': -5,\n",
    "    'trouble': -10,\n",
    "    'falls': -100,\n",
    "}\n",
    "\n",
    "# Instantiate the sentiment intensity analyzer with the already existing lexicon.\n",
    "vader = SentimentIntensityAnalyzer()\n",
    "# Update the lexicon with our new words and values that we want our algorithm to use.\n",
    "vader.lexicon.update(new_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now imported the algorithm we plan to use for our sentiment analysis and updated it with some vocabulary and associated values that we want it to use. We can now prepare our data, using Pandas, and then use our algorithm to generate the polarity scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import pandas.\n",
    "import pandas as pd\n",
    "\n",
    "columns = ['ticker', 'date', 'time', 'headline']\n",
    "# Convert the list of lists we created by parsing the text files into a DataFrame.\n",
    "scored_news = pd.DataFrame(parsed_news)\n",
    "scored_news.columns = columns\n",
    "# We then iterate our algorithm over each headline, generating the polarity scores and storing them in 'score'.\n",
    "scores = [vader.polarity_scores(headline) for headline in scored_news['headline']]\n",
    "# Convert the list of dictionaries we generated into another DataFrame.\n",
    "scores_df = pd.DataFrame(scores)\n",
    "# Join our first DataFrame to this DataFrame, generating a new DataFrame with all of our data, including polarity scores.\n",
    "scored_news = scored_news.join(scores_df)\n",
    "# Convert the date column from string to datetime.\n",
    "scored_news['date'] = pd.to_datetime(scored_news.date).dt.date\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have our data. The next step is cleaning and processing this data to a point that we can use it for our sentiment analysis, which we do by a bit of dataframe manipulation using pandas. We can plot the average sentiment of our tickers against time after this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and prepare modules.\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"fivethirtyeight\")\n",
    "%matplotlib inline\n",
    "\n",
    "# Group by date and ticker columns from scored_news and then calculate the mean.\n",
    "mean_c = (scored_news.groupby(['date', 'ticker'])).mean()\n",
    "# Unstack the column ticker.\n",
    "mean_c = mean_c.unstack('ticker')\n",
    "# Get the cross-section of 'compound' in the 'columns' axis.\n",
    "mean_c = mean_c.xs('compound', axis='columns')\n",
    "# Plot a bar chart with pandas.\n",
    "mean_c.plot.bar()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick look at the bar chart reveals that something is a little off. There is a sharp spike in the graph on November 22nd. Looking back through our data, we can see that there were only 5 headlines from that day, in addition to the fact that two headlines are exactly the same as one another. We need to clean our data up a bit in order to amend these small discrepancies in order to increase the accuracy of the results of our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Count the number of headlines in scored_news (store as integer).\n",
    "num_news_before = len(scored_news)\n",
    "# Drop duplicates based on ticker and headline. This cleans the data.\n",
    "scored_news_clean = scored_news.drop_duplicates(subset=['headline', 'ticker'])\n",
    "# Count number of headlines after dropping duplicates.\n",
    "num_news_after = len(scored_news_clean)\n",
    "# Print before and after numbers to compare the old data to the clean data.\n",
    "f\"Before we had {num_news_before} headlines, now we have {num_news_after}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset is now cleaned and ready for usage. Much can be learned from the dataset. The following code is an example of the type of analysis that can be done with it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set the index to ticker and date.\n",
    "single_day = scored_news_clean.set_index(['ticker', 'date'])\n",
    "# Cross-section the fb row.\n",
    "single_day = single_day.xs('fb', level='ticker')\n",
    "# Select the 3rd of January of 2019.\n",
    "single_day = single_day.loc['2019-01-03']\n",
    "# Convert the datetime string to just the time.\n",
    "single_day['time'] = pd.to_datetime(single_day['time']).dt.time\n",
    "# Set the index to time.\n",
    "single_day = single_day.set_index('time')\n",
    "# Sort it.\n",
    "single_day = single_day.sort_index()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we've extracted a small amount of the data, just some from a single day. We can then plot the data and visualise the positive, negative and neutral scores from one day of trading, according to our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "TITLE = \"Negative, neutral, and positive sentiment for FB on 2019-01-03\"\n",
    "COLORS = [\"red\",\"orange\", \"green\"]\n",
    "# Drop the columns that aren't useful for the plot.\n",
    "plot_day = single_day.drop(['headline', 'compound'])\n",
    "print(plot_day)\n",
    "# Change the column names to 'negative', 'positive', and 'neutral'.\n",
    "plot_day.columns = ['negative', 'positive', 'neutral']\n",
    "# Plot a stacked bar chart.\n",
    "plot_day.plot.bar(stacked = True, \n",
    "                  figsize=(10, 6), \n",
    "                  title = TITLE, \n",
    "                  color = COLORS).legend(bbox_to_anchor=(1.2, 0.5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In summary, we have created a dataset using an NLTK algorithm and news headlines that we extracted from HTML files that, after a bit of cleaning, can be used to analyse the prevailing sentiment of the stock market on Facebook and Tesla stocks. The code uses BeautifulSoup, Pandas and NLTK to extract, store and analyse data in order to achieve this."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
